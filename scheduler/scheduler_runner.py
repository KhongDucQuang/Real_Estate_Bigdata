# File: scheduler/scheduler_runner.py

# --- Code sá»­a sys.path (náº¿u cáº§n, giá»¯ láº¡i tá»« bÆ°á»›c trÆ°á»›c) ---
import sys
import os
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)
# --- Káº¿t thÃºc code sá»­a sys.path ---

import schedule
import time
import signal # Äá»ƒ xá»­ lÃ½ tÃ­n hiá»‡u dá»«ng (Ctrl+C)
import logging
from concurrent.futures import ThreadPoolExecutor

# Import producer vÃ  crawler
# LÆ°u Ã½: Ä‘á»•i láº¡i tÃªn thÆ° má»¥c náº¿u báº¡n dÃ¹ng tÃªn khÃ¡c 'kafka_cc'
from kafka_cc.producer.kafka_producer import send_to_kafka, close_kafka_producer
from crawler.alonhadat import crawl_alonhadat
# from crawler.123nhadatviet import crawl_123nhadatviet # VÃ­ dá»¥ import crawler khÃ¡c

logging.basicConfig(level=logging.INFO)
log = logging.getLogger("scheduler_runner")

# Mapping nguá»“n crawler -> (hÃ m crawl, tÃªn topic)
sources = [
    ("alonhadat", crawl_alonhadat, "alonhadat"), # (TÃªn nguá»“n, HÃ m crawl, TÃªn topic)
    # ("123nhadat", crawl_123nhadat, "123nhadat"), # ThÃªm nguá»“n khÃ¡c á»Ÿ Ä‘Ã¢y
]

def job_for_source(source_name, crawl_func, topic_name):
    log.info(f"ğŸš€ Báº¯t Ä‘áº§u cÃ o dá»¯ liá»‡u tá»«: {source_name}")
    try:
        data = crawl_func() # Gá»i hÃ m crawl tÆ°Æ¡ng á»©ng
        if data is not None: # Kiá»ƒm tra xem cÃ³ dá»¯ liá»‡u tráº£ vá» khÃ´ng
            log.info(f"CÃ o xong {source_name}, thu Ä‘Æ°á»£c {len(data)} báº£n ghi.")
            if data: # Chá»‰ gá»­i náº¿u list khÃ´ng rá»—ng
                send_to_kafka(data, topic_name=topic_name)
        else:
            log.warning(f"HÃ m crawl cá»§a {source_name} khÃ´ng tráº£ vá» dá»¯ liá»‡u.")
    except Exception as e:
        log.error(f"Lá»—i trong quÃ¡ trÃ¬nh cháº¡y job cho {source_name}: {e}", exc_info=True)

def crawl_all_sources():
    log.info("--- Báº¯t Ä‘áº§u chu ká»³ crawl táº¥t cáº£ cÃ¡c nguá»“n ---")
    # Sá»‘ worker báº±ng sá»‘ nguá»“n Ä‘á»ƒ cháº¡y song song tá»‘i Ä‘a
    with ThreadPoolExecutor(max_workers=len(sources)) as executor:
        for name, func, topic in sources:
            # Submit tá»«ng job vÃ o pool
            executor.submit(job_for_source, name, func, topic)
    log.info("--- Káº¿t thÃºc chu ká»³ crawl ---")

# --- Xá»­ lÃ½ tÃ­n hiá»‡u dá»«ng (Ctrl+C) ---
shutdown_signal_received = False
def handle_shutdown_signal(signum, frame):
    global shutdown_signal_received
    if not shutdown_signal_received:
        log.warning(f"Nháº­n Ä‘Æ°á»£c tÃ­n hiá»‡u dá»«ng (Signal {signum}). Äang chá» job hiá»‡n táº¡i hoÃ n thÃ nh vÃ  dá»«ng scheduler...")
        shutdown_signal_received = True
        # KhÃ´ng cancel schedule ngay, Ä‘á»ƒ job Ä‘ang cháº¡y cÃ³ thá»ƒ hoÃ n thÃ nh
        # schedule.clear() # Náº¿u muá»‘n dá»«ng ngay láº­p tá»©c
    else:
        log.warning("Nháº­n tÃ­n hiá»‡u dá»«ng láº§n thá»© hai. Buá»™c dá»«ng.")
        exit(1)

signal.signal(signal.SIGINT, handle_shutdown_signal)  # Ctrl+C
signal.signal(signal.SIGTERM, handle_shutdown_signal) # TÃ­n hiá»‡u dá»«ng tá»« Docker

# --- LÃªn lá»‹ch vÃ  cháº¡y ---
crawl_interval_minutes = 1 # Äáº·t khoáº£ng thá»i gian crawl
log.info(f"Scheduler báº¯t Ä‘áº§u. Sáº½ cháº¡y crawl má»—i {crawl_interval_minutes} phÃºt.")
schedule.every(crawl_interval_minutes).minutes.do(crawl_all_sources)

# Cháº¡y láº§n Ä‘áº§u tiÃªn ngay láº­p tá»©c (tÃ¹y chá»n)
# crawl_all_sources()

# VÃ²ng láº·p chÃ­nh cá»§a scheduler
while not shutdown_signal_received:
    schedule.run_pending()
    time.sleep(1) # Giáº£m táº£i CPU

# --- Dá»n dáº¹p trÆ°á»›c khi thoÃ¡t ---
log.info("Scheduler Ä‘ang dá»«ng. ÄÃ³ng cÃ¡c káº¿t ná»‘i...")
close_kafka_producer()
log.info("Scheduler Ä‘Ã£ dá»«ng.")